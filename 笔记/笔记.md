- self-attention中为什么用sqrt(dk)?
    因为q和k.t()做矩阵相乘会得到一套自注意力矩阵分数，会让结果变的比较尖锐，如果不用sqrt(dk)直接加上softmax会让结果更偏向于0和1，这样在计算softmax梯度的时候会不稳定，不利于反向传播

- self-attention复杂度是O（n^2），制约transforer性能的地方在于softmax，去掉后可以达到linear attention，达到o(n)复杂度。
探究求调softmax是不是有必要？ https://zhuanlan.zhihu.com/p/157490738

- 如何理解selt-attention里面的Q,K,V？
https://www.zhihu.com/question/298810062/answer/1828080188?utm_source=zhihu&utm_medium=social&utm_oi=862674298593230848

- prompt新范式如何选择大模型？
    模型分类：
        1.自回归模型，gpt系列
        2.自编码模型，bert， roberta，deberta
        3.encoder-decoder, T5, bart

- prompt-learning and delta tuning
    delta tuning相当于只tuning大模型很小部分参数也可以达到tuning大模型全参的效果（为什么work，因为pretrain的大模型已经学习到很多
    知识，这个过程只是相当于在激发已经学习到的知识）


常用nlp任务榜单GLUE, SuperGLUE


### 模型总结
- AR(auto regressive)
    缺点：单向的，不能学习到上下文的信息

- AE(auto encoding)
    缺点：预训练使用[MASK], 与下游nlp任务不匹配

- roberta和bert的改进
    - roberta采用动态mask，没次在dataloader前都会随机生成mask矩阵。bert是静态的mask
    - 使用更大的batch_size，加大数据量


- XLNet
    数据量：126G文本， TPU 成本要花 6.14 万美元
    - 融合了AE自编码和AR自回归两种模型的优点， 使用排列语言模型，例如[1,2,3,4]全排列又24种方式，避免使用mask标记的同时可以使用上下文信息
    - Two-Stream Self-Attention，双流自注意力


- T5(Transfer Text-to-Text Transformer):一个统一框架，靠着大力出奇迹，将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务
    - C4数据集，清洗的十分干净
    - 本论文的核心不是提供新方法，而是transformer在transfer learning中提供全局的对比视野

