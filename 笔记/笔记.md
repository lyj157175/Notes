- self-attention中为什么用sqrt(dk)?
    因为q和k.t()做矩阵相乘会得到一套自注意力矩阵分数，会让结果变的比较尖锐，如果不用sqrt(dk)直接加上softmax会让结果更偏向于0和1，这样在计算softmax梯度的时候会不稳定，不利于反向传播


- self-attention复杂度是O（n^2），制约transforer性能的地方在于softmax，去掉后可以达到linear attention，达到o(n)复杂度。
探究求调softmax是不是有必要？ https://zhuanlan.zhihu.com/p/157490738


- 如何理解selt-attention里面的Q,K,V？
https://www.zhihu.com/question/298810062/answer/1828080188?utm_source=zhihu&utm_medium=social&utm_oi=862674298593230848


- prompt新范式如何选择大模型？
    模型分类：
        1.自回归模型，gpt系列
        2.自编码模型，bert， roberta，deberta
        3.encoder-decoder, T5, bart


- prompt-learning and delta tuning
    delta tuning相当于只tuning大模型很小部分参数也可以达到tuning大模型全参的效果（为什么work，因为pretrain的大模型已经学习到很多
    知识，这个过程只是相当于在激发已经学习到的知识）


- BPE算法
    - 使用预料中出现的所有单词，词表过大，会产生oov问题
    - 使用单词做词表会解决oov问题，但粒度过细，使单词失去语义信息


### 激活函数
- relu（transformer）
- gelu（预训练模型主流激活函数，bert， bart)




常用nlp任务榜单GLUE, SuperGLUE

### 模型总结
- 预训练阶段的两种思想
  - AR(auto regressive, 自回归)
      缺点：单向的，不能学习到上下文的信息

  - AE(auto encoding， 自编码) (denoising)
      缺点：预训练使用[MASK], 与下游nlp任务不匹配


- bert
    为什么使用layer norm 而不是batch norm？


- roberta和bert的改进
    - roberta采用动态mask，没次在dataloader前都会随机生成mask矩阵。bert是静态的mask
    - 使用更大的batch_size，加大数据量


- XLNet(2019)
    - 基于自回归的语言建模也可以使用上下文信息。通过排列语言（permutation language modeling）和双流自注意力（Two-Stream Self-Attention）来同时解决
    - 数据量：126G文本， TPU 成本要花 6.14 万美元
    - 融合了AE自编码和AR自回归两种模型的优点， 使用排列语言模型，例如[1,2,3,4]全排列又24种方式，避免使用mask标记的同时可以使用上下文信息
  




- Vanilla Transformer
    transformer的问题在于长句子的输入，例如4000个token一次输入原理上可以，但是attention的计算资源会爆炸
    Vanilla Transformer的改进是进行切片，可以切成8段，每段500个token，对每段进行独立的训练。也即在第二段训练的时候，是看不到第一段的信息
的，这样导致的问题就是「上下文碎片问题（context fragmentation problem）」，由于切片长度限制，模型并不能学习到足够常的依赖信息，这样会
大大损害学习出来的语言模型的性能。
    网络结构没有大的调整，主要引入了辅助损失函数
    片段碎片化，无法利用上下文信息，操作比较复杂，评估速度慢且比较昂贵


- transformer-xl
    解决Vanilla Transformer的一些问题
    提出循环递归片段，相对位置编码
    


- T5(Transfer Text-to-Text Transformer):一个统一框架，靠着大力出奇迹，将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务
    - C4数据集，清洗的十分干净
    - 本论文的核心不是提供新方法，而是transformer在transfer learning中提供全局的对比视野


- bart
    标准的transformer的encoder-decoder架构
    更改relu为gelu
    预训练阶段：先使用多种噪声方式对原始文本进行破坏，然后通过seq2seq模型重建原始文本
    - token masking
    - sentence permutation
    - document rotation
    - token deletion
    - text infilling(效果最好)

    