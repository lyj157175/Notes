- self-attention中为什么用sqrt(dk)?
    因为q和k.t()做矩阵相乘会得到一套自注意力矩阵分数，会让结果变的比较尖锐，如果不用sqrt(dk)直接加上softmax会让结果更偏向于0和1，这样在计算softmax梯度的时候会不稳定，不利于反向传播


- self-attention复杂度是O（n^2），制约transforer性能的地方在于softmax，去掉后可以达到linear attention，达到o(n)复杂度。n是序列长度
探究求调softmax是不是有必要？ https://zhuanlan.zhihu.com/p/157490738


- selt-attention的过程
  attention = { Softmax(Q * K.T) / sqrt(dk) } * V    # dk=hidden_szie/8
  模块输入 x: b, max_len, hidden_size    mask: b, 1, 1, max_len (padding_mask, 为0处设置为极小值)
  x => 进入三个大矩阵Q, K, V (hidden_size, hidden_size)完成线性变换 q, k, v 
    => q, k, v进行分头操作并完成变形得到 b, 8, max_len, hidden_size/8
    => 计算attention权重分数： scores = Softmax(Q * K.T) / sqrt(dk) + mask     # b, 8, max_len, max_len 
    => attention_out = scores * V    # b, 8, max_len, hidden_size/8
    => 维度还原 attention_out.view(b, max_len, hidden)
    => return attention_out, socres  


- 如何理解selt-attention里面的Q,K,V？
https://www.zhihu.com/question/298810062/answer/1828080188?utm_source=zhihu&utm_medium=social&utm_oi=862674298593230848


- prompt新范式如何选择大模型？
    模型分类：
        1.自回归模型，gpt系列
        2.自编码模型，bert， roberta，deberta
        3.encoder-decoder, T5, bart


- transformer和bert为什么要做截断？
  - 传统的Self-Attention机制的时空复杂度与文本的序列长度呈平方的关系，这在很大程度上限制了模型的输入不能太长，因此需要将过长的文档进行截断传入模型进行处理，
  - 例如BERT中能够接受的最大序列长度为512。

- prompt-learning and delta tuning
    delta tuning相当于只tuning大模型很小部分参数也可以达到tuning大模型全参的效果（为什么work，因为pretrain的大模型已经学习到很多
    知识，这个过程只是相当于在激发已经学习到的知识）


- BPE算法
    - 使用预料中出现的所有单词，词表过大，会产生oov问题
    - 使用单词做词表会解决oov问题，但粒度过细，使单词失去语义信息


### 激活函数
- relu（transformer）
- gelu（预训练模型主流激活函数，bert， bart)


### 模型训练
- batch_size的影响 https://zhuanlan.zhihu.com/p/488198704
  batch_size有阈值，尽可能越大越好，超过阈值模型退化


- 模型时间线：
  - transformer（2017.6）
  - gpt（2018.6）
  - bert（2018.10）
  - gpt2（2019.2）
  - gpt3（2020.5）



常用nlp任务榜单GLUE, SuperGLUE

### 模型总结
- 预训练阶段的两种思想
  - AR(auto regressive, 自回归)
      缺点：单向的，不能学习到上下文的信息

  - AE(auto encoding， 自编码) (denoising)
      缺点：预训练使用[MASK], 与下游nlp任务不匹配


- bert
    为什么使用layer norm 而不是batch norm？

- gpt1

- gpt2
  - 15亿参数
  - 卖点：zero-shot，在新颖度上可以，但是有效性差点，所以gpt3来解决gpt2的有效性问题

- gpt3
  - 1750亿参数
  - few-shot时fine-tuning不需要对模型做梯度更新，因为模型太大了，直接使用

- roberta和bert的改进（roberta 160G）
    - roberta采用动态mask，每轮的训练中都会随机生成mask矩阵。bert是静态的mask
    - 使用更大的batch_size，加大数据量,更大的训练步数，去掉nsp任务，采用byte bpe（bert使用的是bpe）

- ernie
  - ERNIE提出了Knowledge Masking的策略，其包含三个级别：ERNIE将Knowledge分成了三个类别：
  - token级别(Basic-Level)、短语级别(Phrase-Level) 和 实体级别(Entity-Level)。通过对这三个级别的对象进行Masking，提高模型对字词、短语的知识理解。


- XLNet(2019)
    - 基于自回归的语言建模也可以使用上下文信息。通过排列语言（permutation language modeling）和双流自注意力（Two-Stream Self-Attention）来同时解决
    - 数据量：126G文本， TPU 成本要花 6.14 万美元
    - 融合了AE自编码和AR自回归两种模型的优点， 使用排列语言模型，例如[1,2,3,4]全排列又24种方式，避免使用mask标记的同时可以使用上下文信息
  

    

- Vanilla Transformer
    transformer的问题在于长句子的输入，例如4000个token一次输入原理上可以，但是attention的计算资源会爆炸
    Vanilla Transformer的改进是进行切片，可以切成8段，每段500个token，对每段进行独立的训练。也即在第二段训练的时候，是看不到第一段的信息
的，这样导致的问题就是「上下文碎片问题（context fragmentation problem）」，由于切片长度限制，模型并不能学习到足够常的依赖信息，这样会
大大损害学习出来的语言模型的性能。
    网络结构没有大的调整，主要引入了辅助损失函数
    片段碎片化，无法利用上下文信息，操作比较复杂，评估速度慢且比较昂贵


- transformer-xl
    解决Vanilla Transformer的一些问题
    提出循环递归片段，相对位置编码

- longformer
  - 解决transformer的self-attention时间复杂度为o（n^2）的问题，n为序列长度
  - https://aistudio.baidu.com/aistudio/projectdetail/2307544


- T5(Transfer Text-to-Text Transformer):一个统一框架，靠着大力出奇迹，将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务
    - C4数据集，清洗的十分干净（745G的数据，需要TPU来训练）
    - 本论文的核心不是提供新方法，而是transformer在transfer learning中提供全局的对比视野


- bart
    标准的transformer的encoder-decoder架构
    更改relu为gelu
    预训练阶段：先使用多种噪声方式对原始文本进行破坏，然后通过seq2seq模型重建原始文本
    - token masking
    - sentence permutation
    - document rotation
    - token deletion
    - text infilling(效果最好)


- k-bert
    将预训练模型中注入知识图谱信息，增加特定领域模型的效果

- electra
  - 利用GAN的思路来训练模型，更少的数据寄更快的训练速度达到roberta的效果
  - ELECTRA 在BERT的基础上对其预训练过程进行了改进：预训练由两部分模型网络组成，称为Generator和Discriminator，各自包含1个BERT模型。
  Generator的预训练使用和BERT一样的Masked Language Model(MLM)任务，但Discriminator的预训练使用Replaced Token Detection(RTD)任务（主要改进点）。
  预训练完成后，使用Discriminator作为精调模型，后续的Fine-tuning不再使用Generator。
  - Generator相当于语言模型，模型输入会随机mask一些单词，然后输出这些mask位置的预测结果。利用Discriminator来二分类判断判断当前token是否被语言模型替换过

- albert
  - a lite bert，共享模型参数，著降低了BERT参数量，同时不损害性能。ALBERT的配置类似于BERT-large，但参数量仅为后者的1/18，训练速度却是bert的1.7倍
  - 两种参数精简技术：
  - 1.词嵌入矩阵进行因式分解，将一个大的词嵌入矩阵分截为两个小矩阵
  - 2.交叉层的参数共享


- HMM模型：解决文本序列的标注问题
  - HMM(A, B, pi)   A,B,pi分别叫做：转移概率矩阵，发射概率矩阵，初始概率矩阵


- simCSE
  - SimCSE（simple contrastive sentence embedding framework），即简单的对比句向量表征框架

- bert+crf
  - https://paddlepedia.readthedocs.io/en/latest/tutorials/natural_language_processing/ner/bilstm_crf.html#1

- 模型蒸馏
  - Distilling Knowledge From Fine-tuned BERT into Bi-LSTM
  - 使用loss = alpha * (ce_loss(logits(bert), logits(bilstm))) + (1-alpha)(mse_loss(logits(bert), logits(bilstm)))
    来联合训练

- 小样本学习（few-shot learning）基本等价元学习（meta learning）


- 学习资料
  1. 封神榜大模型: https://github.com/IDEA-CCNL/Fengshenbang-LM



# NLP tasks
### information extraction

#### NER
  - W2NER(2022): 主要解决扁平，重叠，不连续三种NER常见模式，并在14个数据集上达到sota。模型主要是BERT+bilstm，卷积层，co-predictor层

  - pipline: PURE 陈丹琦

