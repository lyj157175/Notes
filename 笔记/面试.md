- self-attention中为什么用sqrt(dk)?
    因为q和k.t()做矩阵相乘的会让结果变的比较尖锐，如果不用sqrt(dk)直接加上softmax会让结果更偏向于0和1，这样在计算梯度的时候会梯度消失，不利于反向传播



- prompt新范式如何选择大模型？
    模型分类：
        1.自回归模型，gpt系列
        2.自编码模型，bert， roberta，deberta
        3.encoder-decoder, T5, bart

- prompt-learning and delta tuning
    delta tuning相当于只tuning大模型很小部分参数也可以达到tuning大模型全参的效果（为什么work，因为pretrain的大模型已经学习到很多
    知识，这个过程只是相当于在激发已经学习到的知识）

- roberta和bert的改进
    - roberta采用动态mask，没次在dataloader前都会随机生成mask矩阵。bert是静态的mask
    - 使用更大的batch_size，加大数据量