- self-attention中为什么用sqrt(dk)?
    因为q和k.t()做矩阵相乘的会让结果变的比较尖锐，如果不用sqrt(dk)直接加上softmax会让结果更偏向于0和1，这样在计算梯度的时候会梯度消失，不利于反向传播


- prompt新范式如何选择大模型？
    模型分类：
        1.自回归模型，gpt系列
        2.自编码模型，bert， roberta，deberta
        3.encoder-decoder, T5, bart

- prompt-learning and delta tuning
    delta tuning相当于只tuning大模型很小部分参数也可以达到tuning大模型全参的效果（为什么work，因为pretrain的大模型已经学习到很多
    知识，这个过程只是相当于在激发已经学习到的知识）


常用nlp任务榜单GLUE, SuperGLUE

### 模型总结
- roberta和bert的改进
    - roberta采用动态mask，没次在dataloader前都会随机生成mask矩阵。bert是静态的mask
    - 使用更大的batch_size，加大数据量


- XLNet
    数据量：126G文本， TPU 成本要花 6.14 万美元


- T5(Transfer Text-to-Text Transformer):一个统一框架，靠着大力出奇迹，将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务
    - C4数据集，清洗的十分干净
    - 本论文的核心不是提供新方法，而是transformer在transfer learning中提供全局的对比视野

